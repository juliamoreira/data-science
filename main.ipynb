{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwufnHfvfSwv",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Final Report - Data Science\n",
        "## Bachelor's Degree in Computer Science / PUCPR\n",
        "\n",
        "**Prof. Jean Paul Barddal** / **Prof. Rayson Laroca**\n",
        "\n",
        "`Guilherme Schwarz` - `guilherme.schwarz@pucpr.edu.br`\n",
        "\n",
        "`Julia Cristina Moreira da Silva` - `s.moreira4@pucpr.edu.br`\n",
        "\n",
        "`2025`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68-F6cD9VL-J",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Imports & Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "itrIGnaaVQnH",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: librosa in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.11.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: pydub in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.25.1)\n",
            "Requirement already satisfied: soundfile in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.1)\n",
            "Requirement already satisfied: scikit-image in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.25.2)\n",
            "Requirement already satisfied: tensorflow in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.19.0)\n",
            "Requirement already satisfied: sklearn in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.post1)\n",
            "Requirement already satisfied: pandas in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.61.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: pooch>=1.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: networkx>=3.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image) (23.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (5.28.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.28.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.66.2)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: rich in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (3.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chico\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\Chico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Chico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
            "File \u001b[1;32mc:\\Users\\Chico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
            "File \u001b[1;32mc:\\Users\\Chico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\Chico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
          ]
        }
      ],
      "source": [
        "%pip install numpy librosa tqdm pydub soundfile scikit-image tensorflow sklearn pandas\n",
        "\n",
        "import os\n",
        "import time\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTP1pmiPgDY_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Data & Data Treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-VCzmIHcgA2P",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "AUDIO_DIR = 'Data/flac/'\n",
        "PROTOCOL_PATH = 'Data/ASVspoof2021Protocol.txt'\n",
        "OUTPUT_PATH = 'Data/asvspoof_features.csv'\n",
        "SAMPLE_OUTPUT_PATH = 'Data/sample_features.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Treatment Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "\n",
        "def extract_mfcc(y, sr, n_mfcc=40, hop_length=256):\n",
        "    return librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
        "\n",
        "def extract_logmel(y, sr, n_mels=128, hop_length=256):\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
        "    return librosa.power_to_db(mel)\n",
        "\n",
        "def extract_lfcc(y, sr, n_lfcc=40, hop_length=256):\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, hop_length=hop_length)\n",
        "    power_db = librosa.power_to_db(S)\n",
        "    return librosa.feature.mfcc(S=power_db, sr=sr, n_mfcc=n_lfcc)\n",
        "\n",
        "def normalize_feature(x):\n",
        "    return (x - np.min(x)) / (np.max(x) - np.min(x) + 1e-6)\n",
        "\n",
        "def extract_features(y, sr):\n",
        "    mfcc = extract_mfcc(y, sr)\n",
        "    logmel = extract_logmel(y, sr)\n",
        "    lfcc = extract_lfcc(y, sr)\n",
        "    \n",
        "    # Average over time only here\n",
        "    mfcc = np.mean(mfcc, axis=1)\n",
        "    logmel = np.mean(logmel, axis=1)\n",
        "    lfcc = np.mean(lfcc, axis=1)\n",
        "\n",
        "    return np.concatenate([mfcc, logmel, lfcc])\n",
        "\n",
        "def create_feature_image(y, sr, target_shape=(64, 64)):\n",
        "    mfcc = extract_mfcc(y, sr)\n",
        "    logmel = extract_logmel(y, sr)\n",
        "    lfcc = extract_lfcc(y, sr)\n",
        "\n",
        "    # Normalize without reducing dimensions\n",
        "    mfcc = normalize_feature(mfcc)\n",
        "    logmel = normalize_feature(logmel)\n",
        "    lfcc = normalize_feature(lfcc)\n",
        "\n",
        "    # Resize feature maps\n",
        "    mfcc_resized = resize(mfcc, target_shape, mode='reflect', anti_aliasing=True)\n",
        "    logmel_resized = resize(logmel, target_shape, mode='reflect', anti_aliasing=True)\n",
        "    lfcc_resized = resize(lfcc, target_shape, mode='reflect', anti_aliasing=True)\n",
        "\n",
        "    return np.stack([mfcc_resized, logmel_resized, lfcc_resized], axis=-1)\n",
        "\n",
        "def plot_feature_image(image_input):\n",
        "    _, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    titles = ['MFCC', 'Log-Mel', 'LFCC']\n",
        "\n",
        "    for i in range(3):\n",
        "        axs[i].imshow(image_input[:, :, i], aspect='auto', origin='lower', cmap='magma')\n",
        "        axs[i].set_title(titles[i])\n",
        "        axs[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_feature_images(image_input, file_name, output_dir='./Data/Images'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    feature_names = ['mfcc', 'logmel', 'lfcc']\n",
        "\n",
        "    for i, name in enumerate(feature_names):\n",
        "        output_path = os.path.join(output_dir, name)\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(4, 4))\n",
        "        ax.imshow(image_input[:, :, i], aspect='auto', origin='lower', cmap='magma')\n",
        "        ax.axis('off')\n",
        "\n",
        "        fig.patch.set_facecolor('black')\n",
        "        image_path = os.path.join(output_path, f\"{file_name}.png\")\n",
        "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "        plt.savefig(image_path, dpi=100, bbox_inches='tight', pad_inches=0, facecolor='black')\n",
        "        plt.close()\n",
        "\n",
        "def process_audio_row(row, audio_dir):\n",
        "    file_path = os.path.join(audio_dir, row['file_name'] + '.flac')\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "        features = extract_features(y, sr)\n",
        "        feature_image = create_feature_image(y, sr)\n",
        "        save_feature_images(feature_image, row['file_name'])\n",
        "\n",
        "        return {\n",
        "            'file_name': row['file_name'],\n",
        "            'label': 1 if row['label'] == 'bonafide' else 0,\n",
        "            **{f'feature_{i+1}': val for i, val in enumerate(features)}\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Protocol File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>file_name</th>\n",
              "      <th>codec</th>\n",
              "      <th>source_db</th>\n",
              "      <th>system_id</th>\n",
              "      <th>label</th>\n",
              "      <th>trim_status</th>\n",
              "      <th>set_type</th>\n",
              "      <th>spoof_category</th>\n",
              "      <th>track</th>\n",
              "      <th>team</th>\n",
              "      <th>subset</th>\n",
              "      <th>group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LA_0023</td>\n",
              "      <td>DF_E_2000011</td>\n",
              "      <td>nocodec</td>\n",
              "      <td>asvspoof</td>\n",
              "      <td>A14</td>\n",
              "      <td>spoof</td>\n",
              "      <td>notrim</td>\n",
              "      <td>progress</td>\n",
              "      <td>traditional_vocoder</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEF2</td>\n",
              "      <td>DF_E_2000013</td>\n",
              "      <td>low_m4a</td>\n",
              "      <td>vcc2020</td>\n",
              "      <td>Task1-team20</td>\n",
              "      <td>spoof</td>\n",
              "      <td>notrim</td>\n",
              "      <td>eval</td>\n",
              "      <td>neural_vocoder_nonautoregressive</td>\n",
              "      <td>Task1</td>\n",
              "      <td>team20</td>\n",
              "      <td>FF</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TGF1</td>\n",
              "      <td>DF_E_2000024</td>\n",
              "      <td>mp3m4a</td>\n",
              "      <td>vcc2020</td>\n",
              "      <td>Task2-team12</td>\n",
              "      <td>spoof</td>\n",
              "      <td>notrim</td>\n",
              "      <td>eval</td>\n",
              "      <td>traditional_vocoder</td>\n",
              "      <td>Task2</td>\n",
              "      <td>team12</td>\n",
              "      <td>FF</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LA_0043</td>\n",
              "      <td>DF_E_2000026</td>\n",
              "      <td>mp3m4a</td>\n",
              "      <td>asvspoof</td>\n",
              "      <td>A09</td>\n",
              "      <td>spoof</td>\n",
              "      <td>notrim</td>\n",
              "      <td>eval</td>\n",
              "      <td>traditional_vocoder</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LA_0021</td>\n",
              "      <td>DF_E_2000027</td>\n",
              "      <td>mp3m4a</td>\n",
              "      <td>asvspoof</td>\n",
              "      <td>A12</td>\n",
              "      <td>spoof</td>\n",
              "      <td>notrim</td>\n",
              "      <td>eval</td>\n",
              "      <td>neural_vocoder_autoregressive</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     file_name    codec source_db     system_id  label trim_status  \\\n",
              "0  LA_0023  DF_E_2000011  nocodec  asvspoof           A14  spoof      notrim   \n",
              "1     TEF2  DF_E_2000013  low_m4a   vcc2020  Task1-team20  spoof      notrim   \n",
              "2     TGF1  DF_E_2000024   mp3m4a   vcc2020  Task2-team12  spoof      notrim   \n",
              "3  LA_0043  DF_E_2000026   mp3m4a  asvspoof           A09  spoof      notrim   \n",
              "4  LA_0021  DF_E_2000027   mp3m4a  asvspoof           A12  spoof      notrim   \n",
              "\n",
              "   set_type                    spoof_category  track    team subset group  \n",
              "0  progress               traditional_vocoder      -       -      -     -  \n",
              "1      eval  neural_vocoder_nonautoregressive  Task1  team20     FF     E  \n",
              "2      eval               traditional_vocoder  Task2  team12     FF     G  \n",
              "3      eval               traditional_vocoder      -       -      -     -  \n",
              "4      eval     neural_vocoder_autoregressive      -       -      -     -  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "protocol = pd.read_csv(PROTOCOL_PATH, sep=' ', header=None)\n",
        "protocol.columns = [\n",
        "    \"id\", \"file_name\", \"codec\", \"source_db\", \"system_id\", \"label\",\n",
        "    \"trim_status\", \"set_type\", \"spoof_category\",\n",
        "    \"track\", \"team\", \"subset\", \"group\"\n",
        "]\n",
        "\n",
        "protocol.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process 100 files to create a sample and benchmark process (if not already done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:   0%|          | 0/611829 [00:00<?, ?it/s]C:\\Users\\Chico\\AppData\\Local\\Temp\\ipykernel_3924\\1662295131.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(file_path, sr=None)\n",
            "c:\\Users\\Chico\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "Extracting:   0%|          | 4/611829 [00:01<47:34:48,  3.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed 5 files.\n",
            "Total time: 1.12 seconds\n",
            "Average time per file: 0.2232 seconds\n",
            "Expected time for whole feature extraction: 37.92826604962349 hours\n",
            "Sample features saved to 'Data/sample_features.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(SAMPLE_OUTPUT_PATH):\n",
        "    features_list = []\n",
        "    num_files = 3000\n",
        "    processed = 0\n",
        "    total_time = 0\n",
        "\n",
        "    for _, row in tqdm(protocol.iterrows(), total=len(protocol), desc=\"Extracting\"):\n",
        "        start_time = time.time()\n",
        "        result = process_audio_row(row, AUDIO_DIR)\n",
        "        elapsed = time.time() - start_time\n",
        "        if result is not None:\n",
        "            features_list.append(result)\n",
        "            total_time += elapsed\n",
        "            processed += 1\n",
        "        if processed >= num_files:\n",
        "            break\n",
        "\n",
        "    # Save features to CSV\n",
        "    if features_list:\n",
        "        average_time = total_time / processed\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "        features_df.to_csv(SAMPLE_OUTPUT_PATH, index=False)\n",
        "        print(f\"\\nProcessed {processed} files.\")\n",
        "        print(f\"Total time: {total_time:.2f} seconds\")\n",
        "        print(f\"Average time per file: {average_time:.4f} seconds\")\n",
        "        print(f\"Expected time for whole feature extraction: {average_time * len(protocol) / 3600} hours\")\n",
        "        print(f\"Sample features saved to '{SAMPLE_OUTPUT_PATH}'\")\n",
        "    else:\n",
        "        print(\"No features extracted.\")\n",
        "else:\n",
        "    print(\"Sample CSV already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create CSV from audio files (if not already created)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV features file already exists\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(OUTPUT_PATH):\n",
        "    features_list = []\n",
        "    for _, row in tqdm(protocol.iterrows(), total=len(protocol)):\n",
        "        result = process_audio_row(row, AUDIO_DIR)\n",
        "        if result is not None:\n",
        "            features_list.append(result)\n",
        "\n",
        "    if features_list:\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "        features_df.to_csv(OUTPUT_PATH, index=False)\n",
        "        print(f\"Processed {len(features_list)} files. Features saved to '{OUTPUT_PATH}'\")\n",
        "    else:\n",
        "        print(\"No features extracted.\")\n",
        "else:\n",
        "    print(\"CSV features file already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the definitive Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Columns: 210 entries, file_name to feature_208\n",
            "dtypes: float32(208), int64(1), object(1)\n",
            "memory usage: 4.3+ KB\n"
          ]
        }
      ],
      "source": [
        "# features_df will already exist if the code previous code block generated a CSV file.\n",
        "# This checks if the variable was ever created in this session.\n",
        "if \"features_df\" not in locals():\n",
        "    features_df = pd.read_csv(OUTPUT_PATH)\n",
        "\n",
        "features_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>label</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_199</th>\n",
              "      <th>feature_200</th>\n",
              "      <th>feature_201</th>\n",
              "      <th>feature_202</th>\n",
              "      <th>feature_203</th>\n",
              "      <th>feature_204</th>\n",
              "      <th>feature_205</th>\n",
              "      <th>feature_206</th>\n",
              "      <th>feature_207</th>\n",
              "      <th>feature_208</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DF_E_2000011</td>\n",
              "      <td>0</td>\n",
              "      <td>-258.156494</td>\n",
              "      <td>68.461945</td>\n",
              "      <td>-4.070857</td>\n",
              "      <td>41.815220</td>\n",
              "      <td>-12.947649</td>\n",
              "      <td>0.652590</td>\n",
              "      <td>-32.048534</td>\n",
              "      <td>-10.525287</td>\n",
              "      <td>...</td>\n",
              "      <td>-10.158218</td>\n",
              "      <td>-5.305505</td>\n",
              "      <td>-5.499173</td>\n",
              "      <td>-7.173549</td>\n",
              "      <td>-7.561130</td>\n",
              "      <td>-4.460036</td>\n",
              "      <td>-9.567829</td>\n",
              "      <td>-6.674157</td>\n",
              "      <td>-7.094151</td>\n",
              "      <td>-5.831952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DF_E_2000013</td>\n",
              "      <td>0</td>\n",
              "      <td>-197.490753</td>\n",
              "      <td>74.101059</td>\n",
              "      <td>-20.719416</td>\n",
              "      <td>21.303391</td>\n",
              "      <td>3.568402</td>\n",
              "      <td>-23.902008</td>\n",
              "      <td>-21.944942</td>\n",
              "      <td>-15.401686</td>\n",
              "      <td>...</td>\n",
              "      <td>1.075542</td>\n",
              "      <td>2.747469</td>\n",
              "      <td>3.197092</td>\n",
              "      <td>4.042048</td>\n",
              "      <td>2.142642</td>\n",
              "      <td>0.286704</td>\n",
              "      <td>0.267961</td>\n",
              "      <td>-1.313032</td>\n",
              "      <td>-1.234944</td>\n",
              "      <td>-2.650250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DF_E_2000024</td>\n",
              "      <td>0</td>\n",
              "      <td>-245.888214</td>\n",
              "      <td>58.375427</td>\n",
              "      <td>3.938906</td>\n",
              "      <td>27.994375</td>\n",
              "      <td>-4.515254</td>\n",
              "      <td>-2.728295</td>\n",
              "      <td>-31.741747</td>\n",
              "      <td>-4.433678</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.446190</td>\n",
              "      <td>2.675401</td>\n",
              "      <td>-3.271860</td>\n",
              "      <td>3.757226</td>\n",
              "      <td>-0.948672</td>\n",
              "      <td>4.274937</td>\n",
              "      <td>0.186334</td>\n",
              "      <td>1.338551</td>\n",
              "      <td>-2.338968</td>\n",
              "      <td>-0.135259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DF_E_2000026</td>\n",
              "      <td>0</td>\n",
              "      <td>-258.217987</td>\n",
              "      <td>64.792450</td>\n",
              "      <td>-26.174234</td>\n",
              "      <td>31.422873</td>\n",
              "      <td>-39.746807</td>\n",
              "      <td>-4.368946</td>\n",
              "      <td>-24.203897</td>\n",
              "      <td>-17.594040</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.987019</td>\n",
              "      <td>0.448028</td>\n",
              "      <td>-3.692505</td>\n",
              "      <td>-3.039355</td>\n",
              "      <td>-2.499328</td>\n",
              "      <td>-2.017834</td>\n",
              "      <td>-2.915188</td>\n",
              "      <td>-4.044070</td>\n",
              "      <td>-3.990771</td>\n",
              "      <td>-3.360973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DF_E_2000027</td>\n",
              "      <td>0</td>\n",
              "      <td>-285.028107</td>\n",
              "      <td>91.180786</td>\n",
              "      <td>-16.007261</td>\n",
              "      <td>40.066257</td>\n",
              "      <td>-11.105558</td>\n",
              "      <td>-14.232034</td>\n",
              "      <td>-10.550929</td>\n",
              "      <td>-19.979368</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.610543</td>\n",
              "      <td>-4.734384</td>\n",
              "      <td>-3.698732</td>\n",
              "      <td>-8.030618</td>\n",
              "      <td>-7.725413</td>\n",
              "      <td>-3.885382</td>\n",
              "      <td>-1.117573</td>\n",
              "      <td>-2.100805</td>\n",
              "      <td>-4.414304</td>\n",
              "      <td>-8.654475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 210 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      file_name  label   feature_1  feature_2  feature_3  feature_4  \\\n",
              "0  DF_E_2000011      0 -258.156494  68.461945  -4.070857  41.815220   \n",
              "1  DF_E_2000013      0 -197.490753  74.101059 -20.719416  21.303391   \n",
              "2  DF_E_2000024      0 -245.888214  58.375427   3.938906  27.994375   \n",
              "3  DF_E_2000026      0 -258.217987  64.792450 -26.174234  31.422873   \n",
              "4  DF_E_2000027      0 -285.028107  91.180786 -16.007261  40.066257   \n",
              "\n",
              "   feature_5  feature_6  feature_7  feature_8  ...  feature_199  feature_200  \\\n",
              "0 -12.947649   0.652590 -32.048534 -10.525287  ...   -10.158218    -5.305505   \n",
              "1   3.568402 -23.902008 -21.944942 -15.401686  ...     1.075542     2.747469   \n",
              "2  -4.515254  -2.728295 -31.741747  -4.433678  ...    -5.446190     2.675401   \n",
              "3 -39.746807  -4.368946 -24.203897 -17.594040  ...    -0.987019     0.448028   \n",
              "4 -11.105558 -14.232034 -10.550929 -19.979368  ...    -3.610543    -4.734384   \n",
              "\n",
              "   feature_201  feature_202  feature_203  feature_204  feature_205  \\\n",
              "0    -5.499173    -7.173549    -7.561130    -4.460036    -9.567829   \n",
              "1     3.197092     4.042048     2.142642     0.286704     0.267961   \n",
              "2    -3.271860     3.757226    -0.948672     4.274937     0.186334   \n",
              "3    -3.692505    -3.039355    -2.499328    -2.017834    -2.915188   \n",
              "4    -3.698732    -8.030618    -7.725413    -3.885382    -1.117573   \n",
              "\n",
              "   feature_206  feature_207  feature_208  \n",
              "0    -6.674157    -7.094151    -5.831952  \n",
              "1    -1.313032    -1.234944    -2.650250  \n",
              "2     1.338551    -2.338968    -0.135259  \n",
              "3    -4.044070    -3.990771    -3.360973  \n",
              "4    -2.100805    -4.414304    -8.654475  \n",
              "\n",
              "[5 rows x 210 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split data into training, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split features and target\n",
        "X = features_df.drop('label', axis=1)  # Replace with your actual target column name\n",
        "y = features_df['label']\n",
        "\n",
        "# TRAIN      : 70%\n",
        "# VALIDATION : 15%\n",
        "# TEST       : 15%\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Validation performance\n",
        "rf_val_preds = rf_model.predict(X_val)\n",
        "print(\"🎯 Random Forest - Validation Set\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, rf_val_preds))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, rf_val_preds))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, rf_val_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_model = make_pipeline(StandardScaler(), SVC(kernel='rbf', probability=True, random_state=42))\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Validation performance\n",
        "svm_val_preds = svm_model.predict(X_val)\n",
        "print(\"\\n🎯 SVM - Validation Set\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, svm_val_preds))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, svm_val_preds))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, svm_val_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shallow CNN on Logmel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "image_size = (64, 64)\n",
        "batch_size = 32\n",
        "image_dir = './Data/Images/logmel'\n",
        "\n",
        "# Step 1: Image Generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    image_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    image_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Step 2: Define a Shallow CNN\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Train the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8R3Z5xgWyj1i"
      },
      "source": [
        "# Statistical Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ULcpwDnOyj1i"
      },
      "source": [
        "In this section, you should report the key characteristics of the dataset, including but not limited to:\n",
        "* Number of instances;\n",
        "* Number of features;\n",
        "* Number of classes;\n",
        "* Class distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "y5IFTWiTyj1i"
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX9sOXW1gOCU",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Univariate data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQljrblLzQQz",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this section, you should perform univariate data analysis on at least **20 variables**.\n",
        "\n",
        "In the end, you should describe the main variables that are of your interest, and these should be accounted for in the next sections of the report.\n",
        "The definition of each variable chosen should be clarified, so arbitrary selections are **not** accepted at this point.\n",
        "\n",
        "For each variable plotted, make sure you determine the following:\n",
        "1. The distribution of the data (Gaussian, binomial, exponential, etc.);\n",
        "2. Skewness;\n",
        "3. Kurtosis;\n",
        "4. Mean, standard deviation, and what they stand for in the context of the dataset.\n",
        "\n",
        "Ensure that each variable is **plotted correctly** based on its type. For instance, make sure scatterplots are not used for categorical data and so forth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BWS2-FMfgWJR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# place as many cells to plot the visualizations,\n",
        "# as well as to describe the main findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zACQ5EROmhM5",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# if you realize you need to further clean your data here, there is no problem,\n",
        "# yet, make sure you are describing the entire process and the rationale\n",
        "# behind your choices here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAVrOW1tgQU7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Multivariate data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD7VaU40z_sN",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this section, you should plot at least **5 multivariate visualizations**. The key here is to investigate underlying correlations and behaviors within the dataset.\n",
        "Naturally, as visualizations are being created, we should end up with obvious results, yet, you should find at least **ONE** non-obvious behavior in the data.\n",
        "\n",
        "Please follow these steps for creating your visualizations:\n",
        "1. State an hypothesis. Explain why you have selected these specific variables and what you expect to discover through their relationship;\n",
        "2. Determine what kind of visualization is the most suitable;\n",
        "3. Report the findings and discuss whether they corroborate or not the aforementioned hypothesis.\n",
        "\n",
        "\n",
        "### Hints\n",
        "\n",
        "In this section, make sure you go beyond naive explorations. For example, consider applying techniques such as PCA, t-SNE, or even others that we haven't covered in the lectures. The goal is to cultivate a critical mindset toward data analysis and our work.\n",
        "\n",
        "### Important\n",
        "\n",
        "It is strictly prohibited to create multivariate visualizations using variables that were not included in the previous section (univariate data analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fp4ct7TngWmF",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# again, feel free to place as many cells to plot the visualizations,\n",
        "# as well as describe to the main findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehWz8rAcgZ0c",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Final Plots (Effective Data Visualization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHyF3RvU2q9r",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this section, you need to **enhance 3 multivariate visualizations** that were presented in the previous section of the report.\n",
        "The goal is to enhance these visualizations so that they can be effectively presented to an audience unfamiliar with the dataset or with data analysis.\n",
        "**Therefore, make sure that their size, colors, textures, and other visual elements are appropriate and convey the intended information to the audience.**\n",
        "\n",
        "For your final plots, make sure you follow these steps:\n",
        "1. Present the plot;\n",
        "2. Provide a description of the visualization, highlighting the key findings that can be drawn from it.\n",
        "\n",
        "\n",
        "**Hint**: take a look at the checklist based on Evergreen’s work to ensure your visualizations meet the best practices for clarity and impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "K90_0h6JgfnW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua7B5nTmgbN7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Digest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ0cqdoj4Lg7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this section you should write down the main findings of this exploratory data analysis. Furthermore, you should provide a reflection about your own work and effort during the module, highlighting what you believe you have done well and what you should have done differently. This digest should have at least 2,500 characters (excluding spaces)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQWFN0TEPUzp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "```\n",
        "Add your text here.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDlyU2X8yj1j"
      },
      "source": [
        "# Machine Learning (**post checkpoint!**)\n",
        "\n",
        "In this section, you must create at least **3 machine learning models** for the task at hand. Depending on the problem's nature, you must select from classification, regression, or clustering models.\n",
        "It is also important that you:\n",
        "* Select **an appropriate validation protocol**, providing a rationale for why it is appropriate for this specific task;\n",
        "* Choose **a suitable set of evaluation metrics**, providing an explanation for each and describing how it contributes to evaluating the model's performance in the context of this specific task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wmLE99R_yj1k"
      },
      "outputs": [],
      "source": [
        "# use as many cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CaFQEil1F6Q",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Final Steps (Submission)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2R5Kily1H7f",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "1. Save this report as a Jupyter Notebook (`.ipynb`);\n",
        "2. Export a copy of the report as a PDF file (`.pdf`);\n",
        "3. Copy the dataset;\n",
        "4. Compress all the files (the Jupyter Notebook, PDF, and dataset) into a single ZIP archive (`<your_team_name>.zip`);\n",
        "5. Upload the ZIP file to AVA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge-rZWsIyj1k",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
